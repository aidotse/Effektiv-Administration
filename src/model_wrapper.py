from model_wrapper_utils.llama2_chat_hf_wrapper import prompt_model


# TODO
# create a function here, allowing us to choose model, then returning a prompt_model function
# exmaple usage in main.py: prompt_model = initialize_model('llama2-chat... 
    
